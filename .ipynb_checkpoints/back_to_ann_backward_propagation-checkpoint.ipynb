{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into implementing Backward propagation trought time, I decided to implement first backward propagation and then go to the step where we can implement it thought time.\n",
    "\n",
    "I will be using [this](https://blog.zhaytam.com/2018/08/15/implement-neural-network-backpropagation/) tutorial and the any other ressource I will find on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    represent a layer of our neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, neurons, weights=None, biais=None, activation=None):\n",
    "        \"\"\"\n",
    "        n_input : the numbers of input we pass to our network\n",
    "        neurons : the numbers of neurons in this layer\n",
    "        weights : the layer weights\n",
    "        biais   : the layer bias\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weights = weights if weights else np.random.rand(n_input, neurons)\n",
    "        self.biais = biais if biais else np.random.rand(neurons)\n",
    "        self.activation = activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_1 = Layer(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_1.weights.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83244264, 0.21233911, 0.18182497, 0.18340451])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_1.biais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the following activation function to the code :\n",
    "\n",
    "$\\sigma(X*W +B)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(self, x):\n",
    "    dot_product = np.dot(x, self.weights) + self.biais\n",
    "    last_activation = self._apply_activation(dot_product)\n",
    "    return dot_product, last_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer.activate = activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_activation(self, normal):\n",
    "    \"\"\"\n",
    "    apply the activation function to the value pass in parameter\n",
    "    \"\"\"\n",
    "    if self.activation is None:\n",
    "        return normal\n",
    "    elif self.activation == 'tanh':\n",
    "        return np.tanh(normal)\n",
    "    elif self.activation == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(normal))\n",
    "    ### what happen to relu?\n",
    "    return normal\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer._apply_activation = _apply_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64 ==========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.32236508, 3.59926019, 1.09173962, 5.42414484])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_1.activate(np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add our layers to our network and build it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    represent a neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._layers = []\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        add a layer to the network\n",
    "        layer : the layer to add to the network\n",
    "        \"\"\"\n",
    "        self._layers.append(layer)\n",
    "    \n",
    "    def feed_foward(self, X):\n",
    "        for layer in self._layers:\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict a class or class for multi ouput\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs = self.feed_foward(X)\n",
    "        \n",
    "        if outputs.ndim == 1:\n",
    "            return np.argmax(outputs)\n",
    "        return np.argmax(outputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_network = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_network.add_layer(Layer(2, 3, activation='tanh'))\n",
    "the_network.add_layer(Layer(3, 3, activation='sigmoid'))\n",
    "the_network.add_layer(Layer(3, 2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try to see how our network predict a binary operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_network.predict(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_1.weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.48992243, 3.38692108, 0.90991465, 5.24074033]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([[1, 2, 3]]), hidden_layer_1.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase and Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let define a function that calculated sigmoid derivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_derivative(self, output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives \\partial C_x \n",
    "    \\partial a for the output activations.\n",
    "    this is in case our activation function\n",
    "    \"\"\"\n",
    "    return (output_activations-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNetwork.cost_derivative = cost_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation comes from this book on chapter 2 http://neuralnetworksanddeeplearning.com/chap2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(self, x, y):\n",
    "    \"\"\"\n",
    "    return the the tuples with nabla_w, nabla_b representing the gradient of the cost \n",
    "    function layer by layer\n",
    "    which are the derivate of the cost function with the respect to w and b for each layer\n",
    "    \"\"\"\n",
    "    nabla_b = [np.zeros(layer.biais.shape) for layer in self.layers]\n",
    "    nabla_w = [np.zeros(layer.weights.shape) for layer in self.layers]\n",
    "    \n",
    "    # feedforward....\n",
    "    \n",
    "    activation = x\n",
    "    activations = [activation]\n",
    "    z_s = [] # will store z layer by layer \n",
    "    \n",
    "    for layer in self.layers:\n",
    "        z , activation = layer.activate(x)\n",
    "        z.append(z)\n",
    "        activations.append(activation)\n",
    "    delta = self.cost_derivative(activation[-1], y) * sigmoid_prime(z_s[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # from the four equation\n",
    "    \n",
    "    ### back propagate the error \n",
    "    for layer, l in enumerate(reversed(self.layers)):\n",
    "        z = z[-l] # the last z\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(layer.weights, delta) * sp #delta minus one \n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-l-1].transpose()) \n",
    "    return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
