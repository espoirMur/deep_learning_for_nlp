{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, am trying to develop a Recurent Neural Network from scratch,\n",
    "\n",
    "This may sound weird and challenging , am doing it for learning purpose, I want to understand those architecture how they works and how we can use them especially for neural machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory : \n",
    "#### what is a Recurent neural network\n",
    "\n",
    "Recurent neural network or RNN was created to make use of sequentials information.\n",
    "So what are sequentials information, sequentials information are information that comes into a sequence, where the curent sequence depend on the previous sequence.\n",
    "\n",
    "Another way of thinking about recurent neural network it's that they saved in memory what have been calculated so far.\n",
    "\n",
    "They looks like this in the real world \n",
    "\n",
    "![RNN](./pictures/rnn.jpg)*RNN Achitecture [Source](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_t$:  is the input of at time t, it can be a word in a sentence which is modeled as a one hot encoded vector.\n",
    "- $s_t$ : is the hidden state of the network which is calculated from the previous hidden input and the input at current step $s_t = f(Ux_t+Ws_{t-1}))$\n",
    "$f$ is usually a non linear function such as Relu or a tanh\n",
    "- $o_t$ : is the output at time t , for word prediction $o_t$ is an array of all probabilities of word in our corpus.\n",
    "$o_t=softmax(Vs_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some particularities of the network:\n",
    "    \n",
    "    the parameter W, U, V are shared accross all the network in oposition to the feed forward network\n",
    " We do not need always the output at each time , some times we cares only about the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what a neural network can do:\n",
    "    - with this we can predict time series data such as stock exchange prices\n",
    "    - Do Neural Machine Translation, what I care about right now\n",
    "    - Images tagging \n",
    "    - Speech recogniton, it may be useful to check this for local languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see how to implement this stuff now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the now see what are the major component of the arctecturere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what do we do to go from an imput $x_t $ to an output $o_t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we calculate $s_t$, given $s_{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilites functions are :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "def get_params(vocab_size, num_hiddens):\n",
    "    \"\"\"\n",
    "    function help us to return  initialize the neural networks parameters \n",
    "    for text prediction the the number of input and the number of output is equals to the vocabulary size, as well\n",
    "    \"\"\"\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return np.random.normal(scale=0.01, size=shape)\n",
    "    # Hidden layer parameters\n",
    "    U = normal((num_inputs, num_hiddens))\n",
    "    W = normal((num_hiddens, num_hiddens))\n",
    "    # Output layer parameters\n",
    "    H = normal((num_hiddens, num_outputs))\n",
    "    # Attach gradients\n",
    "    Params = namedtuple('Params', [\"U\", \"V\", \"W\"])\n",
    "    params = Params(U=U, V=V, W=W)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_the_hidden_fuction(U, xt, W, s_prev):\n",
    "    \"\"\"\n",
    "    this function calculate the value at st \n",
    "    Args:\n",
    "         U is a matrix\n",
    "         W is a matrix\n",
    "         xt is the input at time t\n",
    "         s_prev the value calculate in the previous layer\n",
    "    \"\"\"\n",
    "    return np.tanh(np.dot(U, xt)+np.dot(W, s_prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ouput_ot(V, st):\n",
    "    \"\"\"\n",
    "    this compute the output at time \n",
    "    Args:\n",
    "        V : it's hyper parameter V which is a matrix\n",
    "        st: the result from the hidden function\n",
    "    \"\"\"\n",
    "    return softmax(np.dot(V, st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 2  functions by themselves will helps us to do the forward pass to the network, we just have to repeat the process n times where n is the number of hiddens units we have in our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let define a forward pass function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, so, output_shape, vocab_size):\n",
    "    \"\"\"\n",
    "    this function will perform a forward pass into our network \n",
    "    args :\n",
    "        x : the input matrix we are using with it shape\n",
    "        so: the intiale state or values of hidden layer :\n",
    "        we fill them with zeroat tht begining\n",
    "        output_shape : how we want our output to look like\n",
    "    \"\"\"\n",
    "    n = x.shape[0] # number of element in the x  we are predicting the next sentence number of hiddens is n\n",
    "    previous_s = [so]\n",
    "    outputs = []\n",
    "    params = get_params(vocab_size, num_hiddens=n)\n",
    "    for i in range(1, n+1):\n",
    "        st = compute_the_hidden_fuction(params.U, x[i-1], params.W, previous_s[i-1])\n",
    "        previous_s.append(st)\n",
    "        output_t = compute_ouput_ot(params.V, st)\n",
    "        outputs.append(output_t)\n",
    "        \n",
    "    return previous_s, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting the data to use fr the training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-10 18:34:31--  https://raw.githubusercontent.com/prateekkarkare/rnn_char_modelling/master/dinos.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.172.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.172.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19909 (19K) [text/plain]\n",
      "Saving to: ‘dinos.txt’\n",
      "\n",
      "dinos.txt           100%[===================>]  19.44K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2020-02-10 18:34:32 (234 KB/s) - ‘dinos.txt’ saved [19909/19909]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/prateekkarkare/rnn_char_modelling/master/dinos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dinos.txt\") as file:\n",
    "    names = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "names = [name.strip() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
