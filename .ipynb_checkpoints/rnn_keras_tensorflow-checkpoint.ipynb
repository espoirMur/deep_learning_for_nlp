{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/es.py/.pyenv/versions/3.6.5/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, rnn_units, input_dim, ouput_dim):\n",
    "        super(MyRNNCell, self).__init__()\n",
    "        self.W_xh = self.add_weight([rnn_units, input_dim]) # from input to hidden layer\n",
    "        self.W_hh = self.add_weight([rnn_units, rnn_units]) # hidden layer to next hidden layer\n",
    "        self.W_hy = self.add_weight([ouput_dim, rnn_units]) # from hidden to output\n",
    "        self.h = tf.zeros([rnn_units, 1])\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        this define the forward pass \n",
    "        \"\"\"\n",
    "        self.h = tf.math.tanh(self.W_hh*self.h + self.W_xh*x) # forward pass\n",
    "        \n",
    "        output = self.W_hy + self.h\n",
    "        \n",
    "        return output, self.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to en_RW@currency=USD.\n",
      "Warning: Failed to set locale category LC_TIME to en_RW@currency=USD.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_RW@currency=USD.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_RW@currency=USD.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_RW@currency=USD.\n",
      "--2020-03-05 13:13:15--  http://www.gutenberg.org/files/35/35-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 204492 (200K) [text/plain]\n",
      "Saving to: ‘timemachine.txt’\n",
      "\n",
      "timemachine.txt     100%[===================>] 199.70K   149KB/s    in 1.3s    \n",
      "\n",
      "2020-03-05 13:13:17 (149 KB/s) - ‘timemachine.txt’ saved [204492/204492]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.gutenberg.org/files/35/35-0.txt -O timemachine.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_time_machine():\n",
    "    \"\"\"Load the time machine book into a list of sentences.\"\"\"\n",
    "    with open('timemachine.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line.strip().lower())\n",
    "            for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_time_machine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentences 3583'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'sentences {len(lines)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"Split sentences into word or char tokens.\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split(' ') for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type '+token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''],\n",
       " ['the',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'ebook',\n",
       "  'of',\n",
       "  'the',\n",
       "  'time',\n",
       "  'machine',\n",
       "  'by',\n",
       "  'h',\n",
       "  'g',\n",
       "  'wells']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize(lines)\n",
    "tokens[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "class Vocab(object):\n",
    "    def __init__(self, tokens, min_freq=0, reserved_tokens=[]):\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[0])\n",
    "        self.token_freqs.sort(key=lambda x: x[1], reverse=True)\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
    "        uniq_tokens += [token for token, freq in self.token_freqs\n",
    "                        if freq >= min_freq and token not in uniq_tokens]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "# Saved in the d2l package for later use\n",
    "def count_corpus(sentences):\n",
    "    # Flatten a list of token lists into a list of tokens\n",
    "    tokens = [tk for line in sentences for tk in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('', 2), ('and', 3), ('of', 4), ('i', 5), ('a', 6), ('to', 7), ('in', 8), ('was', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary is the list of all words we have in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2472), ('', 1400), ('and', 1314), ('of', 1284), ('i', 1268)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_freqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['', 'you', 'must', 'follow', 'me', 'carefully', 'i', 'shall', 'have', 'to', 'controvert', 'one', 'or', 'two']\n",
      "indices: [2, 21, 82, 456, 15, 658, 5, 485, 32, 7, 2902, 37, 23, 171]\n",
      "words: ['ideas', 'that', 'are', 'almost', 'universally', 'accepted', 'the', 'geometry', 'for', 'instance', '']\n",
      "indices: [1362, 10, 74, 181, 4799, 560, 1, 1029, 18, 414, 2]\n"
     ]
    }
   ],
   "source": [
    "for i in range(80, 82):\n",
    "    print('words:', tokens[i])\n",
    "    print('indices:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190638, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    corpus = [vocab[tk] for line in tokens for tk in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus is the list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random sampling :\n",
    "    \n",
    "    Picking sample mini batches will do the trick using random sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    # Offset the iterator over the data for uniform starts\n",
    "    corpus = corpus[random.randint(0, num_steps):]\n",
    "    # Subtract 1 extra since we need to account for label\n",
    "    num_examples = ((len(corpus) - 1) // num_steps)\n",
    "    example_indices = list(range(0, num_examples * num_steps, num_steps))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # This returns a sequence of the length num_steps starting from pos\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    # Discard half empty batches\n",
    "    num_batches = num_examples // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # Batch_size indicates the random examples read each time\n",
    "        batch_indices = example_indices[i:(i+batch_size)]\n",
    "        X = [data(j) for j in batch_indices]\n",
    "        Y = [data(j + 1) for j in batch_indices]\n",
    "        yield np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 31, 32]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(30, 33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[ 5  6  7  8  9 10]\n",
      " [23 24 25 26 27 28]] \n",
      "Y: [[ 6  7  8  9 10 11]\n",
      " [24 25 26 27 28 29]]\n",
      "X:  [[17 18 19 20 21 22]\n",
      " [11 12 13 14 15 16]] \n",
      "Y: [[18 19 20 21 22 23]\n",
      " [12 13 14 15 16 17]]\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X are indexes of words in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 19, 20, 21, 22, 23])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'you', 'this', 'or', 'were', 'on'] ['you', 'this', 'or', 'were', 'on', 'not']\n",
      "['', 'and', 'of', 'i', 'a', 'to'] ['and', 'of', 'i', 'a', 'to', 'in']\n"
     ]
    }
   ],
   "source": [
    "for in_sentence, out_sentence in zip(X, Y) :\n",
    "    print(vocab.to_tokens(list(in_sentence)), vocab.to_tokens(list(out_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for out_sentence in Y :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let get back to this thing and understand text processing before feeding it to a recurent neural network, how we need to process the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let say we have a text, actually a list of sentence , and given a sentence we need to predict the next word in a sentence,  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First problem neural networks don't work with numbers , they work with vectors, how to convvert those text into vector so that our RNN can understand them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some few concepts to understand :\n",
    "    \n",
    "    - Tokenisation : process of breaking down a text into word\n",
    "    - lematisation : removing the ending of a word, likes bat, bats\n",
    "    - stemming : removing suffixes from word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams : \n",
    "    \n",
    "    group of n-tokens in a sentences, let say : natural language processing is awesome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term and document frequency:\n",
    "\n",
    "term frequecny : is the frequency of a word in the current document , \n",
    "document frequency is the measure of how much information the word provides.\n",
    "One hot encoding :\n",
    "\n",
    "One hot encodings are another way of representing words in numeric form. The length of the word vector is equal to the length of the vocabulary, and each observation is represented by a matrix with rows equal to the length of vocabulary and columns equal to the length of observation, with a value of 1 where the word of vocabulary is present in the observation and a value of zero where it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings : [source](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)\n",
    "    word emebedding are text represented into numbers, one of the way to perform word embedding is by using one hot encoding, with a dictionary.\n",
    "    \n",
    "count vector : given a corpus of d document , and n token extracted from the document , count vector is a matrix where the columns are all the  tokens in  the documents, and row are the document themselves. the element of [i][j] is the number of occurence of the token i in the document i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand what are word2vec:\n",
    "    it's not a combinaison of a single algorithm but a combinaison of 2 algorithms :\n",
    "    Continous bag of word and skip gram model.\n",
    "- Cbow (Continous bag of word): a techinque that tend to predict a probabiity of word given a context... the context may be a word or a single group of word...\n",
    "- skip gram : is aims to predict a context given a word...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeding Matrix : [Source](https://qr.ae/p8Lm8h)\n",
    "    To convert a sample into its embedding form, each of the word in its one hot encoded form is multiplied by the embedding matrix to give word embeddings for the sample.\n",
    "Because the problem with one hot vector is sparsity and there are very big but in those vectors most values are 0,\n",
    "and the model can learn unwanted bevaviour that is why we comes up with embeding vectors word to vec : \n",
    "\n",
    ">This is where embedding comes into play. An embedding matrix $W_e \\in R^{K\\times D}$\n",
    "is a linear mapping from the original space (one-of-k) to a real-valued space where entities can have meaningful relationships. Ideally, we wish that we can have\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/natural-language-processing-from-basics-to-using-rnn-and-lstm-ef6779e4ae66\n",
    "https://towardsdatascience.com/what-the-heck-is-word-embedding-b30f67f01c81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The input , we have our corpus or list of sentences , the input is a batch with size m, aka a list of n sentences, each sentence is an array of tokens or words. (This one we know already)*\n",
    "- for this batch the ouput is also a list of word where each word.\n",
    "\n",
    "So if we have this sentence passed to our network :\n",
    "\n",
    "- the time machine by H \n",
    "the output should be :\n",
    "- time machine by H G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check it with this code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_output(X, Y):\n",
    "    for in_sentence, out_sentence in zip(X, Y) :\n",
    "        print('input : ', ' '.join([word for word in vocab.to_tokens(list(in_sentence))])) \n",
    "        print('output: ', ' '.join([word for word in vocab.to_tokens(list(out_sentence))]))\n",
    "        print('='*20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n",
      "(32, 35)\n",
      "(32, 35)\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "i = 0\n",
    "for X, Y in seq_data_iter_random(corpus, batch_size=batch_size, num_steps=num_steps):\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    print('='*20)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object seq_data_iter_random at 0x13a724048>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_data_iter_random(corpus, batch_size=batch_size, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to understand the part building the model dataset:\n",
    "    - https://victorzhou.com/blog/intro-to-rnns/\n",
    "    - https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
