{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, am trying to develop a Recurent Neural Network from scratch,\n",
    "\n",
    "This may sound weird and challenging , am doing it for learning purpose, I want to understand those architecture how they works and how we can use them especially for neural machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory : \n",
    "#### what is a Recurent neural network\n",
    "\n",
    "Recurent neural network or RNN was created to make use of sequentials information.\n",
    "So what are sequentials information, sequentials information are information that comes into a sequence, where the curent sequence depend on the previous sequence.\n",
    "\n",
    "Another way of thinking about recurent neural network it's that they saved in memory what have been calculated so far.\n",
    "\n",
    "They looks like this in the real world \n",
    "\n",
    "![RNN](./pictures/rnn.jpg)*RNN Achitecture [Source](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_t$:  is the input of at time t, it can be a word in a sentence which is modeled as a one hot encoded vector.\n",
    "- $s_t$ : is the hidden state of the network which is calculated from the previous hidden input and the input at current step $s_t = f(Ux_t+Ws_{t-1}))$\n",
    "$f$ is usually a non linear function such as Relu or a tanh\n",
    "- $o_t$ : is the output at time t , for word prediction $o_t$ is an array of all probabilities of word in our corpus.\n",
    "$o_t=softmax(Vs_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some particularities of the network:\n",
    "    \n",
    "    the parameter W, U, V are shared accross all the network in oposition to the feed forward network\n",
    " We do not need always the output at each time , some times we cares only about the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what a neural network can do:\n",
    "    - with this we can predict time series data such as stock exchange prices\n",
    "    - Do Neural Machine Translation, what I care about right now\n",
    "    - Images tagging \n",
    "    - Speech recogniton, it may be useful to check this for local languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see how to implement this stuff now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the now see what are the major component of the arctecturere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what do we do to go from an imput $x_t $ to an output $o_t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we calculate $s_t$, given $s_{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilites functions are :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "def get_params(vocab_size, num_hiddens):\n",
    "    \"\"\"\n",
    "    function help us to return  initialize the neural networks parameters \n",
    "    for text prediction the the number of input and the number of output is equals to the vocabulary size, as well\n",
    "    \"\"\"\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return np.random.normal(scale=0.01, size=shape)\n",
    "    # Hidden layer parameters\n",
    "    U = normal((num_hiddens, num_inputs))\n",
    "    W = normal((num_hiddens, num_hiddens))\n",
    "    # Output layer parameters\n",
    "    V = normal((num_outputs, num_hiddens))\n",
    "    # Attach gradients\n",
    "    Params = namedtuple('Params', [\"U\", \"V\", \"W\"])\n",
    "    params = Params(U=U, V=V, W=W)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_the_hidden_fuction(U, xt, W, s_prev):\n",
    "    \"\"\"\n",
    "    this function calculate the value at st \n",
    "    Args:\n",
    "         U is a matrix\n",
    "         W is a matrix\n",
    "         xt is the input at time t\n",
    "         s_prev the value calculate in the previous layer\n",
    "    \"\"\"\n",
    "    return np.tanh(np.dot(U, xt)+np.dot(W, s_prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ouput_ot(V, st):\n",
    "    \"\"\"\n",
    "    this compute the output at time \n",
    "    Args:\n",
    "        V : it's hyper parameter V which is a matrix\n",
    "        st: the result from the hidden function\n",
    "    \"\"\"\n",
    "    return softmax(np.dot(V, st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 2  functions by themselves will helps us to do the forward pass to the network, we just have to repeat the process n times where n is the number of hiddens units we have in our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let define a forward pass function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, output_shape, vocab_size):\n",
    "    \"\"\"\n",
    "    this function will perform a forward pass into our network \n",
    "    args :\n",
    "        x : the matrix we are passing at one bacth (one sentence)\n",
    "        we fill them with zeroat tht begining\n",
    "        output_shape : how we want our output to look like\n",
    "    \"\"\"\n",
    "    n = x.shape[0] # number of different word in the sentence\n",
    "    outputs = []\n",
    "    params = get_params(vocab_size, num_hiddens=n)\n",
    "    previous_s = [np.zeros((n, ))] # initializinf the previous state with zeros sometimes it's inefficient to use lists\n",
    "    for i in range(0, n):\n",
    "        st = compute_the_hidden_fuction(params.U, x[i, :], params.W, previous_s[i])\n",
    "        previous_s.append(st)\n",
    "        output_t = compute_ouput_ot(params.V, st)\n",
    "        outputs.append(output_t)\n",
    "    return previous_s, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting the data to use fr the training :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already prepared the data in [this notebook](./text_processing.ipynb) I can pick up from where I left things and continue doing my stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(path):\n",
    "    \"\"\"\n",
    "    load the document at the given path\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read()\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_document('data/republic_sentences.txt')\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted',\n",
       " 'i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with',\n",
       " 'went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the',\n",
       " 'down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession',\n",
       " 'yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/es.py/.pyenv/versions/3.6.5/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences= np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can one hot encode our variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sequences[:, :-1], sequences[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([tokenizer.index_word.get(index) for index in X[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delighted'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([tokenizer.index_word.get(index) for index in Y][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = to_categorical(Y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = to_categorical(X, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our input and output set up, let now start with the serious stuffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118632, 50, 7410)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118632, 7410)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let try forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states, outputs = forward_pass(X[1, :, :], Y.shape[1], vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the word predicted to our output without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    \"\"\"\n",
    "    given the input return the predicted output\n",
    "    \"\"\"\n",
    "    hidden_states, outputs = forward_pass(X[0, :, :], Y.shape[1], vocab_size=vocab_size)\n",
    "    return np.argmax(o, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n",
      "break\n",
      "appearances\n",
      "imagines\n",
      "beholding\n",
      "groaning\n",
      "instruct\n",
      "distracted\n",
      "rougher\n",
      "imitating\n",
      "instruct\n",
      "putting\n",
      "separate\n",
      "democratical\n",
      "timid\n",
      "break\n",
      "everything\n",
      "mistress\n",
      "hardness\n",
      "inevitably\n",
      "stingless\n",
      "groaning\n",
      "instruct\n",
      "interdict\n",
      "single\n",
      "instruct\n",
      "narrower\n",
      "continuing\n",
      "seizing\n",
      "comrades\n",
      "staff\n",
      "break\n",
      "substance\n",
      "groaning\n",
      "harshest\n",
      "good\n",
      "cultivation\n",
      "choose\n",
      "purity\n",
      "appurtenances\n",
      "direction\n",
      "instruct\n",
      "incurred\n",
      "husbandmen\n",
      "forgetful\n",
      "now\n",
      "degraded\n",
      "infusion\n",
      "break\n",
      "assure\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    print(tokenizer.index_word.get(np.argmax(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(y, o) = -\\frac{1}{N} \\sum_{n \\in N} y_n log(o_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function we are using is called cross entropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to review why cross entropy and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    N = predictions.shape[0]\n",
    "    values = -np.sum(targets*np.log(predictions))/N\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7410"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(outputs[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001202603771462354"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(outputs[49], Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the best approach ,  from [this](https://gluon.mxnet.io/chapter05_recurrent-neural-networks/simple-rnn.html#Averaging-the-loss-over-the-sequence) tutorial we learn that we need to calculate the loss at each time step and apply it to the whole network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_ce_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = 0.\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_loss += cross_entropy(output, label)\n",
    "    return total_loss / len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each timestep we have the output of our RNN and the expected output from the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step we compare the the ouput and the label at each time step, but what is the label at each time step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label when we pass an input for $x_i$ the correct output is $x_{i+1}$ and the last $x_t$ is $y$ according to how our dataset is structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 7410)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete(X[0], 0, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how to calculate the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.vstack((np.delete(X[1], 0, axis=0), Y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012025256759218995"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_ce_loss(outputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ressources : \n",
    "- https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
    "- https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/\n",
    "- https://gluon.mxnet.io/chapter05_recurrent-neural-networks/simple-rnn.html\n",
    "- https://www.youtube.com/watch?v=RrB605Mbpic A good video for BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
